{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ee_streamflow_prediction_lstm.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wbssdi01/GEE/blob/main/streamflow_prediction_lstm/ee_streamflow_prediction_lstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VB06lpT_mv2X"
      },
      "source": [
        "# Streamflow prediction using EE and LSTM\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gee-community/ee-tensorflow-notebooks/blob/master/streamflow_prediction_lstm/ee_streamflow_prediction_lstm.ipynb)\n",
        "\n",
        "This notebook provides an example workflow of how to access both observed and forcing data for hydrologic modeling and train a Long-Short-Term Memory (LSTM) model for modeling historical streamflow and predicting future streamflow. We use Earth Engine to access meteorological data as inputs into the model.\n",
        "\n",
        "This example is insprired by the following paper: [Rainfallâ€“runoff modelling using Long Short-Term Memory (LSTM) networks](https://www.hydrol-earth-syst-sci.net/22/6005/2018/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARBPrj0UndJH"
      },
      "source": [
        "## Setting up the environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGxVYD0KnekC"
      },
      "source": [
        "### Importing packages\n",
        "\n",
        "Here we use Pandas to handle our time series data that we are going to use to train our model and some HTTP request/IO pacakges to get data from the USGS."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pcc16gBr0yLm"
      },
      "source": [
        "%pylab inline\n",
        "\n",
        "import pandas as pd\n",
        "import requests\n",
        "from io import StringIO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TINhreADGiI"
      },
      "source": [
        "# Import, authenticate and initialize the Earth Engine library.\n",
        "import ee\n",
        "try:\n",
        "    ee.Initialize()\n",
        "except Exception as e:\n",
        "    ee.Authenticate()\n",
        "    ee.Initialize()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjAzgFXv1Bj8"
      },
      "source": [
        "# Tensorflow setup.\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import callbacks\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "# used to help transform data\n",
        "from sklearn import preprocessing\n",
        "\n",
        "print(f\"Using TensorFlow version {tf.__version__}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klT_gSsQHQDc"
      },
      "source": [
        "import folium"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "je7dbY6nrSt8"
      },
      "source": [
        "## Accessing data\n",
        "\n",
        "For this example we will need two datasets:\n",
        "1. Observed streamflow - this can sometimes be difficult to access but the USGS provides an website (https://waterdata.usgs.gov/nwis) to where you can search and even programmitically access gauge data\n",
        "2. Meteorological forcing data - this is meteorological data for a watershed that will be used to model streamflow\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fgInAry1E5G"
      },
      "source": [
        "### Observed streamflow\n",
        "\n",
        "To access the observed streamflow, we will send a request to get a CSV table. We will need to specify a few parameters though including: time period and site number.\n",
        "\n",
        "Time period will be used to get the observed streamflow and forcing data. Whereas the site number is gauge specific and you will need to search and replace this at the `siteNumber` variable. For this example we will use a watershed in the Appalachian Mountain Range, the [Pigeon River Watershed](https://waterdata.usgs.gov/nwis/inventory?agency_code=USGS&site_no=03461500). *FUN FACT*: This area is also a great place to go camping!\n",
        "\n",
        "You can search for a site of interest and accessing the data/gauge information using the [USGS National Water Information System Mapper](https://maps.waterdata.usgs.gov/mapper/index.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yaON7R6DvWQ"
      },
      "source": [
        "# change to your prefered study period\n",
        "# must be within the time range of available observed data\n",
        "START_TIME = '1997-01-01'\n",
        "END_TIME = '2011-12-31'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxgcERDV79sE"
      },
      "source": [
        "# change to the site number of interest\n",
        "siteNumber = '03461500'\n",
        "\n",
        "usgsWaterData = 'https://waterdata.usgs.gov/nwis/dv'\n",
        "params = dict(\n",
        "    site_no=siteNumber,\n",
        "    begin_date=START_TIME, # start time from variable\n",
        "    end_date=END_TIME, # end time from variable\n",
        "    format='rdb', # default parameter, do not change\n",
        ")\n",
        "\n",
        "# send request\n",
        "x = requests.get(usgsWaterData,params=params)\n",
        "\n",
        "if x.status_code == 200:\n",
        "    print('Data request succeeded! ðŸŽ‰')\n",
        "else:\n",
        "    print(\"Uh-oh...something went wrong.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5tzOn6s1_Dt"
      },
      "source": [
        "The returned data comes formatted as text with some header information so first we will have to parse the header from the actual data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHW70prU8Az0"
      },
      "source": [
        "# split the response text into the header information and table data\n",
        "header,table = x.text.split('# \\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kTyLtw49aCP"
      },
      "source": [
        "print(header)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mi2t6uYZ3Cfs"
      },
      "source": [
        "After we have parsed the data from the response, we can load it into a Pandas DataFrame for processing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIdj4ozz9dd0"
      },
      "source": [
        "colNames = ['agency','site','datetime','discharge','quality']\n",
        "df = pd.read_csv(StringIO(table),sep='\\t')\n",
        "df = df.iloc[1:]\n",
        "df.columns = colNames\n",
        "\n",
        "print(f'Table dimensions: {df.shape}')\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_5Rn0Mv3M-w"
      },
      "source": [
        "Metric units are used all over the world (except the US...) and in science so we will convert the units from cubic feet per second to cubic meters per second.\n",
        "Also we will index the DataFrame by the `datetime` information so we can do some time series processing on it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atkI3m7lADoW"
      },
      "source": [
        "df.index = pd.to_datetime(df['datetime'])\n",
        "df = df['discharge'].astype(np.float32) * 0.0283168 # convert to m^3/s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HSfzlIJ3kQ4"
      },
      "source": [
        "## Forcing data\n",
        "\n",
        "Our next step is to access forcing data, this is typically meteorological data. Earth Engine provides great functionality to access vast amounts of met data and format it in a way that can directly be used for this application. To do this we will need the geometry of the watershed and the image collection of met data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbduNQb64x8x"
      },
      "source": [
        "Here we specify the location of the gauge station and select from the HydroSheds FeatureCollection which one intersects with the gauge.\n",
        "\n",
        "*CAUTION*: Typically for lumped hydrologic modeling you will want the gauge be as close to the watershed outlet as possible. If you change the watershed of interest be sure to carefully select which level of HydroSheds makes sense."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0Gb4dsEGQRi"
      },
      "source": [
        "# specify where the gauge is located so we can filter the basin by location\n",
        "gaugeLat,gaugeLon = 35.96055556, -83.17444444\n",
        "gaugeGeom = ee.Geometry.Point([gaugeLon,gaugeLat])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Tcpym6s1D4t"
      },
      "source": [
        "hydrosheds = ee.FeatureCollection(\"WWF/HydroSHEDS/v1/Basins/hybas_8\")\n",
        "\n",
        "pigeonRiverBasin = hydrosheds.filterBounds(gaugeGeom)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAK4C6Sl5RHm"
      },
      "source": [
        "Just so we can see where we are at in the world, let's map the watershed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jhd9KMy_HmLw"
      },
      "source": [
        "basinOutline = ee.Image().byte()\\\n",
        "    .paint(\n",
        "        featureCollection= pigeonRiverBasin,\n",
        "        color= 1,\n",
        "        width= 3\n",
        "    ).getMapId()\n",
        "\n",
        "map = folium.Map(location=[gaugeLat,gaugeLon],zoom_start=9,height=750)\n",
        "folium.TileLayer(\n",
        "    tiles=basinOutline['tile_fetcher'].url_format,\n",
        "    attr='Google Earth Engine',\n",
        "    overlay=True,\n",
        "    name='Pigeon River Basin',\n",
        "  ).add_to(map)\n",
        "\n",
        "folium.Marker([gaugeLat,gaugeLon]).add_to(map)\n",
        "\n",
        "map.add_child(folium.LayerControl())\n",
        "map"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMhzoi265nVm"
      },
      "source": [
        "Now we need to get the meterological image collection, for this we will use the [daily ERA5 dataset](https://developers.google.com/earth-engine/datasets/catalog/ECMWF_ERA5_DAILY) and select the precipitation, minimum/maximum temperature, and wind variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFy7VZMrzDbC"
      },
      "source": [
        "# specify band names we want\n",
        "metBands = ['total_precipitation','minimum_2m_air_temperature','maximum_2m_air_temperature','u_component_of_wind_10m','v_component_of_wind_10m']\n",
        "\n",
        "# filter the collection by date and select the bands on interest\n",
        "era5 = ee.ImageCollection('ECMWF/ERA5/DAILY')\\\n",
        "    .filterDate(START_TIME,ee.Date(END_TIME).advance(1,'day'))\\\n",
        "    .select(metBands)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doNv7Mrq6Umq"
      },
      "source": [
        "All that is left to do is get the data. `imageCollection.getRegion()` is a handy function that allows us to request all of the data over a geometry from an imageCollection. Then we use `.getInfo()` to get the data on the client side.\n",
        "\n",
        "*CAUTION*: When using `.getInfo()` you can easily run into memory limit errors. One way to avoid this is to limit your time period you are looking at or request data at a larger scale."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WZyy0E6JrIg"
      },
      "source": [
        "scale = 30000 # request scale in meters\n",
        "lumpedForcings = era5.getRegion(pigeonRiverBasin,scale).getInfo()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOJW1ZTZ6saP"
      },
      "source": [
        "The resulting data comes in a specific format that will need to be organized. Lukily, pandas handles this nicely so we will create a DataFrame of the pixel information."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMREa-WjyaMG"
      },
      "source": [
        "forcingDf = pd.DataFrame(lumpedForcings[1:])\n",
        "forcingDf.columns = lumpedForcings[0]\n",
        "forcingDf.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zFLsjXl67a4"
      },
      "source": [
        "If there are more than one pixels per time period then we will need to aggregate to create a mean value for each variable per time period. We do this using `.groupby()` by the imageId and specifying statistics to aggregate by.\n",
        "\n",
        "Also, we will do some manipulation to the dataframe to make it a time series and name the columns something a little more intuitive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LG1znZsyNH9u"
      },
      "source": [
        "forcingDf = forcingDf.groupby(by='id')\\\n",
        "    .agg({'total_precipitation':'mean',\n",
        "          'minimum_2m_air_temperature':'mean',\n",
        "          'maximum_2m_air_temperature':'mean',\n",
        "          'u_component_of_wind_10m':'mean',\n",
        "          'v_component_of_wind_10m':'mean',\n",
        "          'time':'mean'\n",
        "         }) \n",
        "\n",
        "forcingDf.index = pd.to_datetime(forcingDf['time']*1e6)\n",
        "forcingDf.index.name = 'datetime'\n",
        "forcingDf.drop(['time'],axis=1,inplace=True)\n",
        "\n",
        "newCols = ['precip','tmin','tmax','uwind','vwind']\n",
        "forcingDf.columns = newCols"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEtd5Jn9Nn7L"
      },
      "source": [
        "forcingDf.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MoKrRUG8ijL"
      },
      "source": [
        "Great, nicely formatted! Now we can concatenate both the observed streamflow and forcing data into a single dataframe. This will be handy when we have to format the data for the LSTM model to keep track of everything together."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQyxZhZHQWij"
      },
      "source": [
        "modelDf = pd.concat([df,forcingDf],axis=1)\n",
        "modelDf.dropna(inplace=True)\n",
        "\n",
        "modelDf.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWVPAoCP8z4a"
      },
      "source": [
        "## Building and training the model\n",
        "\n",
        "Now we have the data we can start working on our LSTM model. LSTM's take data in a spefic format that support the time component of the model: [samples,features,time].\n",
        "\n",
        "Now we have samples and features but we will need to create lagged arrays along a third dimension to provide the time component. Here we define a custom function to do this for use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wU8hzbHq1Hij"
      },
      "source": [
        "def dataPrep(df,featureNames,labelNames,timeLag=10,predLead=0,scalingFunc=None,pctTrain=0.77):\n",
        "    \"\"\"\n",
        "    Function to pred dataframe for input into a RNN\n",
        "\n",
        "    Args:\n",
        "        df: dataframe with features and label\n",
        "        featureNames: list of column names that will be the input features\n",
        "        labelNames: list of column names that will be the output labels\n",
        "    Kwargs:\n",
        "        timeLag: time to lag datasets default = 10 periods\n",
        "        predLead: time period as forecast ouputs\n",
        "        scalingFunc: sklearn.preprocessing function to preprocess features\n",
        "        pctTrain: percent of data to be used for training, used to prevent scaling on all features\n",
        "    Returns:\n",
        "        X_train: array of training features\n",
        "        X_test: array of testing features\n",
        "        y_train: array of training labels\n",
        "        y_test: array of testing labels\n",
        "    \"\"\"\n",
        "\n",
        "    # get features\n",
        "    x = df[featureNames].values\n",
        "\n",
        "    # get the labels\n",
        "    if predLead > 0:\n",
        "        y = df[labelNames][timeLag:-predLead].values\n",
        "    else:\n",
        "         y = df[labelNames][timeLag:].values\n",
        "\n",
        "    nTrain = int(pctTrain*x.shape[0])\n",
        "\n",
        "    # if a scaling function is provided then scale the data\n",
        "    if scalingFunc is not None:\n",
        "        scaler = scalingFunc.fit(x[:nTrain])\n",
        "        x_scaled = scaler.transform(x)\n",
        "    else:\n",
        "        x_scaled = x\n",
        "\n",
        "    xshape = [y.shape[0]]+[x.shape[1]]+[timeLag]\n",
        "    yshape = [y.shape[0],predLead] if predLead > 0 else [y.shape[0],1]\n",
        "    outx = np.zeros(xshape)\n",
        "    outy = np.zeros(yshape)\n",
        "    for i in range(y.shape[0]-predLead):\n",
        "        v = timeLag+i if i >0 else timeLag\n",
        "        u = predLead+i if i>0 else predLead\n",
        "        u = u if predLead > 0 else i+1\n",
        "        outx[i,:,:] = x_scaled[i:v,:].T\n",
        "        outy[i,:] = y[i:u].T\n",
        "\n",
        "    nTrain = int(pctTrain*outx.shape[0])\n",
        "\n",
        "    xtrain,xtest = outx[:nTrain,:,:],outx[nTrain:,:,:]\n",
        "    ytrain,ytest = outy[:nTrain,:],outy[nTrain:,:]\n",
        "\n",
        "    return xtrain,xtest,ytrain,ytest\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txldwobc_6_K"
      },
      "source": [
        "Now we have a function to prepare our data. Let's use it.\n",
        "\n",
        "Here we define some of the parameters that will be passed in our `dataPred()` function. We use 365 days as input into the model, all the met data, and 85% of the data for training. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mV5keXL0MIVW"
      },
      "source": [
        "timeDim = 365\n",
        "featureColumns = [\"precip\",\"tmin\",\"tmax\",\"uwind\",\"vwind\"]\n",
        "labelColumns = ['discharge']\n",
        "pctTrain = 0.85\n",
        "\n",
        "scaler = preprocessing.RobustScaler()\n",
        "\n",
        "X_train,X_test,y_train,y_test = dataPrep(modelDf,featureColumns,labelColumns,timeLag=timeDim,scalingFunc=scaler,pctTrain=pctTrain)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlrqROZbA4Wy"
      },
      "source": [
        "Let's check to make sure the data is properly formatted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x46XFdpx1IHD"
      },
      "source": [
        "print(X_train.shape,y_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDvZzDGaA8y_"
      },
      "source": [
        "We can see that the `X_train` array does in fact have 3 dimensions with the samples, features, and time information and our `y_train` array matches the samples length."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F50coeZxBWOK"
      },
      "source": [
        "Let's next define a function to build our model. This model will be a simple LSTM model with two LSTM layers with ReLU activations and an output layer. The function will compile the model and return it ready to use. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbWn3BeJ1NNH"
      },
      "source": [
        "def buildModel(inshape,outshape,nodes=32,optimizer='adam',loss='mse'):\n",
        "    \"\"\"\n",
        "    Function to build out LSTM model\n",
        "\n",
        "    Args:\n",
        "        inshape: list or Tensor defining input shape for model must have [features,time]\n",
        "        outshape: int defining output shape for model must have [time]\n",
        "    Kwargs:\n",
        "        nodes: number of nodes to use in LSTM layers, default = 32\n",
        "        optimizer: string or optimizer to use for model, default = adam optimizer\n",
        "        loss: string of loss function to use for model, default = mean squared error\n",
        "    Returns: \n",
        "        model: compiled Keras model ready for training\n",
        "    \"\"\"\n",
        "    inputs = layers.Input(inshape,name=\"input_layer\")\n",
        "    x = layers.LSTM(nodes,return_sequences=True,name=\"lstm_layer_1\",activation=\"relu\")(inputs)\n",
        "    x = layers.LSTM(nodes,return_sequences=False,name=\"lstm_layer_2\",activation=\"relu\")(x)\n",
        "    outputs = layers.Dense(outshape,activation=\"linear\",name=\"output_layer\")(x)\n",
        "\n",
        "    model = models.Model(inputs=[inputs],outputs=[outputs],name=\"hydroNet\")\n",
        "\n",
        "    model.compile(optimizer=optimizer,\n",
        "                 loss=loss,\n",
        "                 metrics=[\"mape\",\"mae\",\"mse\"])\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAyayXvAC64O"
      },
      "source": [
        "In hydrologic modeling it is commom to use a metric called the Nash-Sutcliffe model efficiency coefficient (NSE). So, here we define a custom loss function to calculate NSE and since the NSE values range from -$\\infty$ to 1 where 1 is a perfect model. However, for loss functions lower values are better so we will invert the values so that -1 is best."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oE8OZMIE1LC1"
      },
      "source": [
        "def nse_loss(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Custom metric function to calculate the Nash-Sutcliffe model efficientcy coefficient\n",
        "    From: https://en.wikipedia.org/wiki/Nash%E2%80%93Sutcliffe_model_efficiency_coefficient\n",
        "    Commonly used in hydrology to evaluate a model performance (NSE > 0.7 is good)\n",
        "    \n",
        "    Args:\n",
        "        y_true: Tensor with true values from observations/labels\n",
        "        y_pred: Tensor of predicted values from model\n",
        "    Returns: \n",
        "       tf.Tensor of the inverted NSE value\n",
        "    \"\"\"\n",
        "    numer = K.sum(K.pow(y_true-y_pred,2))\n",
        "    denom = K.sum(K.pow(y_true-K.mean(y_true),2)) + K.epsilon()\n",
        "    nse = (1 - (numer/denom))\n",
        "    return -1*nse\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isF5tzwWEMXQ"
      },
      "source": [
        "Now we can build our model. We define the input and output shape parameters and provide the custom `nse_loss` function for the loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-L_kkN51Pc4"
      },
      "source": [
        "inshape = len(featureColumns),timeDim\n",
        "outshape = 1\n",
        "\n",
        "model = buildModel(inshape=inshape,outshape=outshape,loss=nse_loss)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTcGUPR9BN_L"
      },
      "source": [
        "EPOCHS = 15\n",
        "STEPS_PER_EPOCH = 500\n",
        "BATCH_SIZE = X_train.shape[0]//STEPS_PER_EPOCH\n",
        "VAL_SPLIT = 0.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V219R9Eg1RiB"
      },
      "source": [
        "training = model.fit(x=X_train,y=y_train,\n",
        "                     epochs=EPOCHS,\n",
        "                     batch_size=BATCH_SIZE,\n",
        "                     validation_split=VAL_SPLIT)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYooYGyeJC9L"
      },
      "source": [
        "We have a trained model so now let's look at what happened during training. Here we plot the history and inspect the loss and some metric values for each epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1LQiCKC6GNz"
      },
      "source": [
        "fig, ax = plt.subplots(nrows=2, sharex=True, figsize=(10,5.5))\n",
        "\n",
        "ax[0].plot(training.history['loss'],color='#1f77b4',label='Training Loss')\n",
        "ax[0].plot(training.history['val_loss'],linestyle=':',marker='o',markersize=3,color='#1f77b4',label='Validation Loss')\n",
        "ax[0].set_ylabel('Loss')\n",
        "ax[0].legend()\n",
        "\n",
        "ax[1].plot(training.history['mae'],color='#ff7f0e',label='MAE')\n",
        "ax[1].plot(training.history['val_mae'],linestyle=':',marker='o',markersize=3,color='#ff7f0e',label='Validation MAE')\n",
        "ax[1].set_ylabel('Mean Absolute Error')\n",
        "ax[1].set_xlabel('Epoch')\n",
        "ax[1].legend(loc=\"lower right\")\n",
        "\n",
        "ax[1].set_xticks(range(1,len(training.epoch)+1,5))\n",
        "ax[1].set_xticklabels(range(1,len(training.epoch)+1,5))\n",
        "ax[1].set_xlabel('Epoch')\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBj-SvA4JPyG"
      },
      "source": [
        "In addition to inpecting the training histoy, we can also predict the streamflow from the test samples and evaluate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPingqm61c5A"
      },
      "source": [
        "# apply the prediction\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# drop the extra dimension for the prediction and test arrays\n",
        "# this is done for plotting\n",
        "y_pred = np.squeeze(y_pred) \n",
        "y_test = np.squeeze(y_test) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9uu94trLbUd"
      },
      "source": [
        "# calculate some metrics to see how well are predictions are doing\n",
        "rmse = np.mean(np.sqrt(np.power((y_test-y_pred),2)))\n",
        "nse = 1-(np.sum(np.power((y_test-y_pred),2))/np.sum(np.power((y_test-y_test.mean()),2)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BET13rRLAAT"
      },
      "source": [
        "Now we can plot our results to see inspect how our model performs on data it has not seen before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2lkmYjD1g1A"
      },
      "source": [
        "print(f'NSE: {nse:.4f}  RMSE: {rmse:.4f}')\n",
        "\n",
        "eval_dates = modelDf.iloc[X_train.shape[0]+timeDim:].index\n",
        "\n",
        "fig = plt.figure(figsize=(25,8))\n",
        "gs = fig.add_gridspec(1, 3)\n",
        "\n",
        "ax1 = fig.add_subplot(gs[0, :2])\n",
        "ax1.plot(eval_dates, y_pred,label='Predicted')\n",
        "ax1.plot(eval_dates, y_test,label='Observed')\n",
        "ax1.set_xlabel('Date')\n",
        "ax1.set_ylabel('Discharge')\n",
        "ax1.legend(fontsize=12)\n",
        "\n",
        "ax2 = fig.add_subplot(gs[0, 2:])\n",
        "ax2.plot(y_pred,y_test,'o',alpha=0.4)\n",
        "ax2.plot([0,y_test.max()],[0,y_test.max()],'k--',alpha=0.75)\n",
        "ax2.set_xlabel('Predicted')\n",
        "ax2.set_ylabel('Observed')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RE6lWCPIOMl-"
      },
      "source": [
        "Alright, not too bad for a hindcast. There are plenty of ways to customized the model to increase accuracy. This serves largely as an small example to get started and there are plenty of ways to customize for your own application."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LI09MyglEbKW"
      },
      "source": [
        "## Forecasting streamflow\n",
        "\n",
        "We have seen an example of how we can use an LSTM for generating already observed streamflow but what if we want to forecast? We can do that too, all it takes is a little adjustment for the input data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V28dppKgQZcF"
      },
      "source": [
        "To perfrom a forecast, we will specify a forecastLead variable which will be number of days to forecast and adjust our `timeLag` to 60 days (most forecasts are not influenced by more than two months worth of data). Then we get our training and testing data with the new dimensions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQRRuEkzzWIk"
      },
      "source": [
        "forecastLead = 15\n",
        "timeDim = 60\n",
        "\n",
        "X_train,X_test,y_train,y_test = dataPrep(modelDf,featureColumns,labelColumns,timeLag=timeDim,predLead=forecastLead,scalingFunc=scaler,pctTrain=pctTrain)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_46SKBsEeTg"
      },
      "source": [
        "# check the shape matches what we would expect\n",
        "print(X_train.shape,y_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05sHEgw4anhf"
      },
      "source": [
        "We can now build the forecast model with and provide the dimensions of the input data. We are going to use the same model build from earlier but provide a few more nodes in each LSTM layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pNMbSojEjRR"
      },
      "source": [
        "inshape = len(featureColumns),timeDim\n",
        "outshape = forecastLead\n",
        "\n",
        "model = buildModel(inshape=inshape,outshape=outshape,nodes=48,loss=nse_loss)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtQe0sjka2sv"
      },
      "source": [
        "Now we fit the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VtDMhMyEuKm"
      },
      "source": [
        "model.fit(x=X_train,y=y_train,\n",
        "          epochs=EPOCHS,\n",
        "          batch_size=BATCH_SIZE,\n",
        "          validation_split=VAL_SPLIT)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tonope53FnRd"
      },
      "source": [
        "# run the predictions\n",
        "forecast = model.predict(X_test)\n",
        "forecast.shape # print the shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwHdVDMjbDRt"
      },
      "source": [
        "Alright, we have the model and prediction outputs from the model. Now let's see how it worked...but looking at time series forecasts is not straightforward!\n",
        "\n",
        "We will use some IPython widget action to help us out. Here we define a function to plot our forecasts. We use a date picker to help visualize the forecast for that day then plot the historical streamflow with the predicted and true streamflow based on the forecast date."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJ4rXhGgFrGD"
      },
      "source": [
        "from ipywidgets import *\n",
        "\n",
        "eval_dates = modelDf.iloc[X_train.shape[0]+timeDim:].index\n",
        "initDate = datetime.datetime(2010,5,1)\n",
        "\n",
        "@widgets.interact(date=DatePicker(description='Pick a Date:',value=initDate))\n",
        "def plotUpdate(date):\n",
        "    fIdx = int(np.where(eval_dates==str(date))[0])\n",
        "    fig,ax = plt.subplots(1,1,figsize=(15,8))\n",
        "    historicLine, = ax.plot(eval_dates[fIdx-50:fIdx],y_test[fIdx-50:fIdx,0],label=\"historic\",alpha=0.5)\n",
        "    predictedLine, = ax.plot(eval_dates[fIdx:fIdx+forecastLead],forecast[fIdx,:],color=\"C1\", marker='o',label=\"predicted\")\n",
        "    truthLine, = ax.plot(eval_dates[fIdx:fIdx+forecastLead],y_test[fIdx:fIdx+forecastLead,0],color=\"C0\", marker='o',label=\"truth\")\n",
        "    ax.set_title(f'Forecast for {date}',fontsize=12)\n",
        "    ax.set_ylabel('Discharge [m^3/s]')\n",
        "    plt.legend(fontsize=10)\n",
        "    plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sy_MkS5ZUggg"
      },
      "source": [
        "Based on these results, I would not be putting any money on the predictions.... Again, this serves more as an example rather than a production ready model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suckAdmOb5Il"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "In this example we looked at how we can use Earth Engine and TensorFlow to predict streamflow. We accessed observed streamflow from USGS, used Earth Engine to get meteorological data for our watershed, and built LSTM models for hindcast and forecasts of streamflow. While these examples do not produce the best results, they serve as a starting point to where you can create streamflow predictions using deep learning. If you would want to run this in production then you could write functions to call the data from EE and run the predictions in real-time!"
      ]
    }
  ]
}