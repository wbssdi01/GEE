{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lc8_ee_qa_unet.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wbssdi01/GEE/blob/main/landsat_qa_cnn/lc8_ee_qa_unet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTn7SAZ3l9FI"
      },
      "source": [
        "# Landsat Quality Assessment Mask Generation using U-Net\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gee-community/ee-tensorflow-notebooks/blob/master/landsat_qa_cnn/lc8_ee_qa_unet.ipynb)\n",
        "\n",
        "This notebook provides a workflow to export training and validation data to build a deep learning model for QA masks with Landsat 8. The model will predict Cloud, Shadow, Snow, Water, Clear, and No Data classes in Landsat imagery."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrPDt_XAmFsD"
      },
      "source": [
        "## Setting up the environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOKSNcUxm9lz"
      },
      "source": [
        "### Importing packages and checking versions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6F4TL_6zPHzL"
      },
      "source": [
        "import os\n",
        "# Cloud authentication.\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "# get numpy and matplotlib.pyplot\n",
        "%pylab inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbcs-UX7_QW-"
      },
      "source": [
        "# check to see what type of GPU is available\n",
        "# best is Tesla P100-PCIE-16GB\n",
        "!nvidia-smi -L"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgcgnC_Lg8CB"
      },
      "source": [
        "# Import, authenticate and initialize the Earth Engine library.\n",
        "import ee\n",
        "try:\n",
        "    ee.Initialize()\n",
        "except Exception as e:\n",
        "    ee.Authenticate()\n",
        "    ee.Initialize()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "In5ATyzdOn7M"
      },
      "source": [
        "# Tensorflow setup.\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import callbacks\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_ixFWWln9RN"
      },
      "source": [
        "# Folium setup.\n",
        "import folium\n",
        "print(folium.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_r7UQOpSnDc8"
      },
      "source": [
        "### Declaring global variables that will be used throughout the workflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXHWe48-k8e9"
      },
      "source": [
        "# Specify cloud storage bucket to save data too\n",
        "BUCKET = 'ee-rsqa'\n",
        "\n",
        "# Specify names locations for outputs in Cloud Storage. \n",
        "FOLDER = 'training_data'\n",
        "TRAINING_BASE = 'training_patches'\n",
        "TESTING_BASE = 'testing_patches'\n",
        "VAL_BASE = 'val_patches'\n",
        "\n",
        "# Specify inputs (Landsat bands) to the model and the response variable.\n",
        "opticalBands = ['B2', 'B3', 'B4', 'B5', 'B6', 'B7']\n",
        "BANDS = opticalBands\n",
        "RESPONSE = ['cloud','shadow','snow','water','land','nodata']\n",
        "FEATURES = BANDS + RESPONSE\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzTL-nAXNrOx"
      },
      "source": [
        "# specify a kernel/image size to use for the model\n",
        "KERNEL_SIZE = 256\n",
        "\n",
        "# create an EE kernel opject from the kernel size\n",
        "list = ee.List.repeat(1, KERNEL_SIZE)\n",
        "lists = ee.List.repeat(list, KERNEL_SIZE)\n",
        "kernel = ee.Kernel.fixed(KERNEL_SIZE, KERNEL_SIZE, lists)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1_9nHEHn8Xx"
      },
      "source": [
        "## Exporting training data to GCS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XO_HkJjDomvW"
      },
      "source": [
        "### Loading the Earth Engine assests\n",
        "Here we use the Landsat 8 surface reflectance product and a hand labeled dataset, the [SPARCS dataset](https://www.usgs.gov/land-resources/nli/landsat/spatial-procedures-automated-removal-cloud-and-shadow-sparcs-validation)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqz-uNMIKplT"
      },
      "source": [
        "# function to rescale LC8 data to 0-1 range\n",
        "def rescale(img):\n",
        "    return img.divide(10000).copyProperties(img).set('system:time_start',img.date())\n",
        "\n",
        "# Use Landsat 8 surface reflectance data.\n",
        "l8sr = ee.ImageCollection('LANDSAT/LC08/C01/T1_SR').map(rescale)\n",
        "# get image collection of hand labeled data and rename the bands\n",
        "sparcs = ee.ImageCollection('projects/gmap/datasets/manual_qaMasks/sparcs_masks').select(['b1','b2','b3','b4','b5'],RESPONSE[:-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UytBXPcoggN"
      },
      "source": [
        "### Visualizing an example image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tQ_sFPBK0Ep"
      },
      "source": [
        "# grab the first image in the SPARCS image collection\n",
        "maskImage = ee.Image(sparcs.first())\n",
        "ndImage = ee.Image.constant(1).where(maskImage.reduce(ee.Reducer.sum()).eq(1),0).rename('nodata')\n",
        "labelImage = ee.Image.cat([ndImage,maskImage])\n",
        "\n",
        "# Filter the landsat collection that overlaps with the labeled image.\n",
        "l8Image = l8sr.filterDate(maskImage.date().advance(-1,'hour'),maskImage.date().advance(1,'hour'))\\\n",
        "  .filterBounds(maskImage.geometry())\\\n",
        "  .mosaic()\n",
        "\n",
        "# Use folium to visualize the imagery.\n",
        "l8mapid = l8Image.getMapId({'bands': ['B7', 'B5', 'B3'], 'min': 0.05, 'max': 0.55, 'gamma':1.5})\n",
        "qamapid = labelImage.getMapId({'bands': ['cloud','land','nodata'], 'max': 1})\n",
        "label = ndImage.getMapId({'bands': ['nodata'], 'max': 1})\n",
        "\n",
        "map = folium.Map(location=[-30.4853,-71.5639])\n",
        "folium.TileLayer(\n",
        "    tiles=l8mapid['tile_fetcher'].url_format,\n",
        "    attr='Google Earth Engine',\n",
        "    overlay=True,\n",
        "    name='Landsat Image',\n",
        "  ).add_to(map)\n",
        "\n",
        "folium.TileLayer(\n",
        "    tiles=qamapid['tile_fetcher'].url_format,\n",
        "    attr='Google Earth Engine',\n",
        "    overlay=True,\n",
        "    name='QA Mask',\n",
        "  ).add_to(map)\n",
        "\n",
        "map.add_child(folium.LayerControl())\n",
        "map"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXZhKAARplRt"
      },
      "source": [
        "### Exporting the training dataset\n",
        "Here we loop through all of the labeled datasets, colocate with landsat imagery, and export the features as TFRecords to the cloud storage bucket we specified earlier.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44t7dDcBNeqq",
        "cellView": "both"
      },
      "source": [
        "# These numbers determined experimentally.\n",
        "n = 10 # Number of shards in each polygon.\n",
        "N = 150 # Total sample size in each image.\n",
        "\n",
        "nImages = sparcs.size().getInfo() # total number of sparcs images\n",
        "\n",
        "# convert label data image collection to list\n",
        "sparcsList = sparcs.toList(nImages)\n",
        "\n",
        "# Export all the training data (in many pieces), with one task \n",
        "# per geometry.\n",
        "for i in range(nImages):\n",
        "    # get the ith labeled image\n",
        "    qaImage = ee.Image(sparcsList.get(i))\n",
        "\n",
        "    # add a no data band\n",
        "    # this is needed to avoid training issues with image edges where data will be 0 for all bands\n",
        "    ndImage = ee.Image.constant(1).where(qaImage.reduce(ee.Reducer.sum()).gt(0),0).rename('nodata')\n",
        "    labelImage = ee.Image.cat([ndImage,qaImage])\n",
        "    \n",
        "    # grab the landsat image that corresponds with the labeled data\n",
        "    l8Image = l8sr.filterDate(qaImage.date().advance(-1,'hour'),qaImage.date().advance(1,'hour'))\\\n",
        "        .filterBounds(qaImage.geometry())\\\n",
        "        .mosaic()\\\n",
        "        .mask(ndImage.Not())\n",
        "\n",
        "    # combine the labels and features\n",
        "    exampleStack = ee.Image.cat([\n",
        "        l8Image.select(BANDS),\n",
        "        labelImage.select(RESPONSE)\n",
        "    ]).float()\n",
        "    # conver to a neighborhood array\n",
        "    arrays = exampleStack.neighborhoodToArray(kernel)\n",
        "\n",
        "    # sample n times within image and add to feature collection\n",
        "    # n should be great enough to allow for image overlap\n",
        "    # we oversample to flip, rotate, and augment the data during training\n",
        "    geomSample = ee.FeatureCollection([])\n",
        "    for j in range(n):\n",
        "        sample = arrays.sample(\n",
        "        region = qaImage.geometry(), \n",
        "        scale = 30, \n",
        "        numPixels = N / n, # Size of the shard.\n",
        "        seed = j**2,\n",
        "        tileScale = 8\n",
        "        )\n",
        "        geomSample = geomSample.merge(sample)\n",
        "\n",
        "    # add random column to feature and split between training, test, and validataion\n",
        "    geomSample = geomSample.randomColumn('random',i)\n",
        "    training = geomSample.filter(ee.Filter.lt('random',0.6))\n",
        "    testing = geomSample.filter(ee.Filter.rangeContains('random',0.6,0.84))\n",
        "    validation = geomSample.filter(ee.Filter.gte('random',0.85))\n",
        "  \n",
        "    # set up the training export and start\n",
        "    desc = TRAINING_BASE + '_i' + str(i)\n",
        "    task = ee.batch.Export.table.toCloudStorage(\n",
        "        collection = training,\n",
        "        description = desc, \n",
        "        bucket = BUCKET, \n",
        "        fileNamePrefix = FOLDER + '/' + desc,\n",
        "        fileFormat = 'TFRecord',\n",
        "        selectors = FEATURES\n",
        "    )\n",
        "    task.start()\n",
        "\n",
        "    # set up the testing export and start\n",
        "    desc = TESTING_BASE + '_i' + str(i)\n",
        "    task = ee.batch.Export.table.toCloudStorage(\n",
        "        collection = testing,\n",
        "        description = desc, \n",
        "        bucket = BUCKET, \n",
        "        fileNamePrefix = FOLDER + '/' + desc,\n",
        "        fileFormat = 'TFRecord',\n",
        "        selectors = FEATURES\n",
        "    )\n",
        "    task.start()\n",
        "\n",
        "    # set up the validation export and start\n",
        "    desc = VAL_BASE + '_i' + str(i)\n",
        "    task = ee.batch.Export.table.toCloudStorage(\n",
        "        collection = validation,\n",
        "        description = desc, \n",
        "        bucket = BUCKET, \n",
        "        fileNamePrefix = FOLDER + '/' + desc,\n",
        "        fileFormat = 'TFRecord',\n",
        "        selectors = FEATURES\n",
        "    )\n",
        "    task.start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wziVa2pNq44b"
      },
      "source": [
        "## Building and training the Convolutional Neural Network (CNN)\n",
        "\n",
        "This next section focuses on building and training a CNN that performs the predictions.\n",
        "\n",
        "We will use the precreated CNN VGG-16 as the encoder branch of our network and create a custom decoder branch. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfnKTVhWrJpd"
      },
      "source": [
        "### Defining and compiling the network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGEpkPI0rIaR"
      },
      "source": [
        "# custom decoder block to upsample the features in the network\n",
        "# this specific decoder block uses a cov2d -> concat -> conv2d * n -> bilinear upsample\n",
        "def decoder_block(input_tensor, concat_tensor=None, nFilters=512,nConvs=2,i=0,name_prefix=\"decoder_block\"):\n",
        "    deconv = input_tensor\n",
        "    for j in range(nConvs):\n",
        "        deconv = layers.Conv2D(nFilters, 3, activation='relu',\n",
        "                               padding='same',name=f\"{name_prefix}{i}_deconv{j+1}\")(deconv)\n",
        "        deconv = layers.BatchNormalization(name=f\"{name_prefix}{i}_batchnorm{j+1}\")(deconv)\n",
        "        if j == 0:\n",
        "            if concat_tensor is not None:\n",
        "                 deconv = layers.concatenate([deconv,concat_tensor],name=f\"{name_prefix}{i}_concat\")\n",
        "            deconv = layers.Dropout(0.2, seed=0+i,name=f\"{name_prefix}{i}_dropout\")(deconv)\n",
        "    \n",
        "    up = layers.UpSampling2D(interpolation='bilinear',name=f\"{name_prefix}{i}_upsamp\")(deconv)\n",
        "    return up"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vu9kEjiyofi"
      },
      "source": [
        "# here we define the network using the VGG-16 encoder \n",
        "# and build our decoder from there\n",
        "\n",
        "# specify an input tensor with an arbitrary shape for x and y dims\n",
        "# has sample length channels as landsat bands we exported\n",
        "inTensor = layers.Input(shape=[None,None,len(BANDS)],name=\"input\")\n",
        "\n",
        "# grab the vgg-16 encoder and build based off our input tensor\n",
        "vgg16 = keras.applications.VGG19(include_top=False,weights=None,input_tensor=inTensor)\n",
        "\n",
        "# grab the input and output tensors\n",
        "base_in = vgg16.input\n",
        "base_out = vgg16.output\n",
        "\n",
        "# extract the tensors we will use to concatenate our decoders with\n",
        "concat_layers = [\"block5_conv3\",\"block4_conv3\",\"block3_conv3\",\"block2_conv2\",\"block1_conv2\"]\n",
        "concat_tensors = [vgg16.get_layer(layer).output for layer in concat_layers]\n",
        "\n",
        "# define the decoder branch\n",
        "\n",
        "decoder0 = decoder_block(base_out, nFilters=512,nConvs=1,i=0) # center block with no upsampling\n",
        "decoder1 = decoder_block(decoder0, concat_tensor=concat_tensors[0], nFilters=512,nConvs=1,i=2) \n",
        "decoder2 = decoder_block(decoder1, concat_tensor=concat_tensors[1], nFilters=256,nConvs=1,i=3) \n",
        "decoder3 = decoder_block(decoder2, concat_tensor=concat_tensors[2], nFilters=128,nConvs=1,i=4) \n",
        "decoder4 = decoder_block(decoder3, concat_tensor=concat_tensors[3], nFilters=64,nConvs=1,i=5) \n",
        "# concat the final decoder block with the first encoder output\n",
        "# drop out correlated connections in spatial space\n",
        "outBranch = layers.concatenate([decoder4,concat_tensors[4]],name=\"out_block_concat1\")\n",
        "outBranch = layers.SpatialDropout2D(rate=0.2,seed=0,name=\"out_block_spatialdrop\")(outBranch)\n",
        "\n",
        "# perform some additional convolutions before predicting probabilites\n",
        "outBranch = layers.Conv2D(64, 3, activation='relu', \n",
        "                          padding='same',name=\"out_block_conv1\")(outBranch)\n",
        "outBranch = layers.BatchNormalization(name=\"out_block_batchnorm1\")(outBranch)\n",
        "outBranch = layers.Conv2D(64, 3, activation='relu', \n",
        "                          padding='same',name=\"out_block_conv2\")(outBranch)\n",
        "outBranch = layers.BatchNormalization(name=\"out_block_batchnorm2\")(outBranch)\n",
        "# final convolution and softmax activation to get output probabilities\n",
        "# nodes will equal the number of classes\n",
        "outBranch = layers.Conv2D(len(RESPONSE), (1, 1),name='final_conv')(outBranch)\n",
        "output = layers.Activation(\"softmax\",name=\"final_out\")(outBranch)\n",
        " \n",
        "# declare our model with the inputs from the encoder and outputs from the decoder\n",
        "model = models.Model(inputs=[base_in], outputs=[output],name=\"vgg16-unet\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEqwJmz_g0VC"
      },
      "source": [
        "# this is were we compile the model but first define some custom functions for\n",
        "# metric monitoring and a custom loss function\n",
        "\n",
        "# custom recall function\n",
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "# custom precision function\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "# custom F1-score function\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
        "\n",
        "# soft dice loss function\n",
        "# based on https://arxiv.org/pdf/1707.03237.pdf \n",
        "def dice_loss(y_true, y_pred, smooth=1):\n",
        "    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
        "    true_sum = K.sum(K.square(y_true),-1) \n",
        "    pred_sum = K.sum(K.square(y_pred),-1)\n",
        "    return 1 - ((2. * intersection + smooth) / (true_sum + pred_sum + smooth))\n",
        "\n",
        "# define an adaptive learning rate based on training\n",
        "lr_schedule = keras.optimizers.schedules.InverseTimeDecay(\n",
        "  0.001,\n",
        "  decay_steps=500,\n",
        "  decay_rate=1,\n",
        "  staircase=False)\n",
        "\n",
        "# compile the model\n",
        "# uses Adam loss with adaptive learning rate\n",
        "# soft dice loss as opjective function\n",
        "# outputs accuracy, precision, recall, and f1\n",
        "model.compile(optimizer=keras.optimizers.Adam(lr_schedule),\n",
        "              loss=dice_loss,\n",
        "              metrics=[keras.metrics.categorical_accuracy,\n",
        "                       precision_m,\n",
        "                       recall_m,\n",
        "                       f1_m])\n",
        "\n",
        "# display the model summary to see layers and parameters\n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bU7X4Tt9v_sO"
      },
      "source": [
        "### Defining training data pipeline\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1qtFP5CXmY8"
      },
      "source": [
        "# Sizes of the training and evaluation datasets.\n",
        "# based on sizes of exported data and spliting performed earlier\n",
        "# ~80 total images with 150 samples per image = ~12000 samples\n",
        "# ~65% are training, ~25% are testing, ~10% are validation\n",
        "TRAIN_SIZE = 7800 \n",
        "TEST_SIZE =  3000\n",
        "VAL_SIZE = 1200 \n",
        "\n",
        "# Specify model training parameters.\n",
        "BATCH_SIZE = 20\n",
        "EPOCHS = 20 \n",
        "BUFFER_SIZE = 3000 # setting too large will give an Out of Memory (OOM) error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTZBEGemKjAQ"
      },
      "source": [
        "# Specify the size and shape of patches expected by the model.\n",
        "KERNEL_SHAPE = [KERNEL_SIZE, KERNEL_SIZE]\n",
        "COLUMNS = [\n",
        "  tf.io.FixedLenFeature(shape=KERNEL_SHAPE, dtype=tf.float32) for k in FEATURES\n",
        "]\n",
        "FEATURES_DICT = dict(zip(FEATURES, COLUMNS))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7xwu-O-GOLB"
      },
      "source": [
        "def parse_tfrecord(example_proto):\n",
        "    \"\"\"The parsing function.\n",
        "    Read a serialized example into the structure defined by FEATURES_DICT.\n",
        "    Args:\n",
        "    example_proto: a serialized Example.\n",
        "    Returns: \n",
        "    A dictionary of tensors, keyed by feature name.\n",
        "    \"\"\"\n",
        "    return tf.io.parse_single_example(example_proto, FEATURES_DICT)\n",
        "\n",
        "\n",
        "def to_tuple(inputs):\n",
        "    \"\"\"Function to convert a dictionary of tensors to a tuple of (inputs, outputs).\n",
        "    Turn the tensors returned by parse_tfrecord into a stack in HWC shape.\n",
        "    Args:\n",
        "    inputs: A dictionary of tensors, keyed by feature name.\n",
        "    Returns: \n",
        "    A dtuple of (inputs, outputs).\n",
        "    \"\"\"\n",
        "    inputsList = [inputs.get(key) for key in FEATURES]\n",
        "    stacked = tf.stack(inputsList, axis=0)\n",
        "    # Convert from CHW to HWC\n",
        "    stacked = tf.transpose(stacked, [1, 2, 0])\n",
        "    return stacked[:,:,:len(BANDS)], stacked[:,:,len(BANDS):]\n",
        "\n",
        "# custom function to randomly augment the data during training\n",
        "def transform(features,labels):\n",
        "    x = tf.random.uniform(())\n",
        "    # flip image on horizontal axis\n",
        "    if x < 0.12: \n",
        "        feat = tf.image.flip_left_right(features)\n",
        "        labl = tf.image.flip_left_right(labels)\n",
        "    # flip image on vertical axis\n",
        "    elif tf.math.logical_and(x >=0.12, x < 0.24):\n",
        "        feat = tf.image.flip_up_down(features)\n",
        "        labl = tf.image.flip_up_down(labels)\n",
        "    # transpose image on bottom left corner\n",
        "    elif tf.math.logical_and(x >=0.24, x < 0.36):\n",
        "        feat = tf.image.flip_left_right(tf.image.flip_up_down(features))\n",
        "        labl = tf.image.flip_left_right(tf.image.flip_up_down(labels))\n",
        "    # rotate to the left 90 degrees\n",
        "    elif tf.math.logical_and(x >=0.36, x < 0.48):\n",
        "        feat = tf.image.rot90(features,k=1)\n",
        "        labl = tf.image.rot90(labels,k=1)\n",
        "    # rotate to the left 180 degrees\n",
        "    elif tf.math.logical_and(x >=0.48, x < 0.60):\n",
        "        feat = tf.image.rot90(features,k=2)\n",
        "        labl = tf.image.rot90(labels,k=2)\n",
        "    # rotate to the left 270 degrees\n",
        "    elif tf.math.logical_and(x >=0.60, x < 0.72):\n",
        "        feat = tf.image.rot90(features,k=3)\n",
        "        labl = tf.image.rot90(labels,k=3)\n",
        "    # transpose image on bottom right corner\n",
        "    elif tf.math.logical_and(x >=0.72, x < 0.84):\n",
        "        feat = tf.image.flip_left_right(tf.image.rot90(features,k=2))\n",
        "        labl = tf.image.flip_left_right(tf.image.rot90(labels,k=2))\n",
        "    else:\n",
        "        feat = features\n",
        "        labl = labels\n",
        "    \n",
        "    return feat,labl\n",
        "\n",
        "def get_dataset(pattern,training=False):\n",
        "    \"\"\"Function to read, parse and format to tuple a set of input tfrecord files.\n",
        "    Get all the files matching the pattern, parse and convert to tuple.\n",
        "    Args:\n",
        "    pattern: A file pattern to match in a Cloud Storage bucket.\n",
        "    Returns: \n",
        "    A tf.data.Dataset\n",
        "    \"\"\"\n",
        "    glob = tf.gfile.Glob(pattern)\n",
        "    dataset = tf.data.TFRecordDataset(glob, compression_type='GZIP')\n",
        "    dataset = dataset.map(parse_tfrecord, num_parallel_calls=5)\n",
        "    dataset = dataset.map(to_tuple, num_parallel_calls=5)\n",
        "    if training:\n",
        "        dataset = dataset.map(transform)\n",
        "    return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_pTNo5WGO3b"
      },
      "source": [
        "\n",
        "def get_training_dataset():\n",
        "    \"\"\"Get the preprocessed training dataset\n",
        "    Returns: \n",
        "    A tf.data.Dataset of training data.\n",
        "    \"\"\"\n",
        "    glob = 'gs://' + BUCKET + '/' + FOLDER + '/' + 't*'\n",
        "    dataset = get_dataset(glob,training=True)\n",
        "    dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
        "    return dataset\n",
        "\n",
        "training = get_training_dataset()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmWcd6pZGTG6"
      },
      "source": [
        "def get_testing_dataset():\n",
        "\t\"\"\"Get the preprocessed evaluation dataset\n",
        "  Returns: \n",
        "    A tf.data.Dataset of evaluation data.\n",
        "  \"\"\"\n",
        "\tglob = 'gs://' + BUCKET + '/' + FOLDER + '/' + VAL_BASE + '*'\n",
        "\tdataset = get_dataset(glob)\n",
        "\tdataset = dataset.batch(1).repeat()\n",
        "\treturn dataset\n",
        "\n",
        "testing = get_testing_dataset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkH7jOtHx_t4"
      },
      "source": [
        "## Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyHbBhUjNSfa"
      },
      "source": [
        "# define a callback to stop training is the validation loss does not improve after 3 epochs\n",
        "earlyStopping = callbacks.EarlyStopping(monitor='val_loss', patience=3, mode='min', restore_best_weights=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Zo_C0OUhA0K"
      },
      "source": [
        "# train the model!!!\n",
        "history = model.fit(x=training,\n",
        "                    epochs=EPOCHS,\n",
        "                    steps_per_epoch=(TRAIN_SIZE // BATCH_SIZE),\n",
        "                    validation_data=testing,\n",
        "                    validation_steps=TEST_SIZE,\n",
        "                    callbacks=[earlyStopping],\n",
        "                    initial_epoch=0,\n",
        "                   )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCwfsrq50Doa"
      },
      "source": [
        "Save the model to disc so we can use it later"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1OYsvWKHwh-"
      },
      "source": [
        "# save the trained model to a file and move to your GCS bucket\n",
        "model.save(\"vgg16unet_model.h5\")\n",
        "!gsutil cp vgg16unet_model.h5 gs://{BUCKET}/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cKhp5D3yL4D"
      },
      "source": [
        "### Displaying results of training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-6gUGuyoVsz"
      },
      "source": [
        "# plot the results of model training\n",
        "\n",
        "fig, ax = plt.subplots(nrows=2, sharex=True, figsize=(10,5.5))\n",
        "\n",
        "ax[0].plot(history.history['loss'],color='#1f77b4',label='Training Loss')\n",
        "ax[0].plot(history.history['val_loss'],linestyle=':',marker='o',markersize=3,color='#1f77b4',label='Validation Loss')\n",
        "ax[0].set_ylabel('Loss')\n",
        "ax[0].set_ylim(0,0.15)\n",
        "ax[0].legend()\n",
        "\n",
        "ax[1].plot(history.history['categorical_accuracy'],color='#ff7f0e',label='Training Acc.')\n",
        "ax[1].plot(history.history['val_categorical_accuracy'],linestyle=':',marker='o',markersize=3,color='#ff7f0e',label='Validation Acc.')\n",
        "ax[1].set_ylabel('Accuracy')\n",
        "ax[1].set_xlabel('Epoch')\n",
        "ax[1].legend(loc=\"lower right\")\n",
        "\n",
        "ax[1].set_xticks(history.epoch)\n",
        "ax[1].set_xticklabels(range(1,len(history.epoch)+1))\n",
        "ax[1].set_xlabel('Epoch')\n",
        "ax[1].set_ylim(0.8,1)\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "# plt.savefig(\"/content/drive/My Drive/landsat_qa_samples/training.png\",dpi=300,)\n",
        "\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SWEeAmNyPnk"
      },
      "source": [
        "### Validating model results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2KxSTHinVr8"
      },
      "source": [
        "def get_validation_dataset():\n",
        "    \"\"\"Get the preprocessed training dataset\n",
        "    Returns: \n",
        "    A tf.data.Dataset of training data.\n",
        "    \"\"\"\n",
        "    glob = 'gs://' + BUCKET + '/' + FOLDER + '/' + VAL_BASE + '*'\n",
        "    dataset = get_dataset(glob)\n",
        "    dataset = dataset.batch(1)\n",
        "    return dataset\n",
        "\n",
        "validation = get_validation_dataset()\n",
        "\n",
        "# evaluate the model\n",
        "model.evaluate(validation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWIKzsz0bws7"
      },
      "source": [
        "## Deploying the trained model to AI Platform \n",
        "\n",
        "Here we save the model and deploy to AI platform so we can use it within Earth Engine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-acKQtaNzYZq"
      },
      "source": [
        "# save the model as a TF Estimator which is what \n",
        "MODEL_NAME = 'vgg16-unet'\n",
        "TF_DIR = 'gs://{}/{}/'.format(BUCKET,MODEL_NAME)\n",
        "\n",
        "# tf.keras.models.save_model(model,TF_DIR,save_format='tf')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yA-pZrWaCZcG"
      },
      "source": [
        "from tensorflow.python.tools import saved_model_utils\n",
        "\n",
        "meta_graph_def = saved_model_utils.get_meta_graph_def(TF_DIR, 'serve')\n",
        "inputs = meta_graph_def.signature_def['serving_default'].inputs\n",
        "outputs = meta_graph_def.signature_def['serving_default'].outputs\n",
        "\n",
        "# Just get the first thing(s) from the serving signature def.  i.e. this\n",
        "# model only has a single input and a single output.\n",
        "input_name = None\n",
        "for k,v in inputs.items():\n",
        "    input_name = v.name\n",
        "    break\n",
        "\n",
        "output_name = None\n",
        "for k,v in outputs.items():\n",
        "    output_name = v.name\n",
        "    break\n",
        "\n",
        "# Make a dictionary that maps Earth Engine outputs and inputs to \n",
        "# AI Platform inputs and outputs, respectively.\n",
        "import json\n",
        "input_dict = \"'\" + json.dumps({input_name: \"array\"}) + \"'\"\n",
        "output_dict = \"'\" + json.dumps({output_name: 'qa'}) + \"'\"\n",
        "print(input_dict)\n",
        "print(output_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngab17Wo1SvF"
      },
      "source": [
        "### Creating an EEified model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "goJqQfLi6E7Q"
      },
      "source": [
        "# Put the EEified model next to the trained model directory.\n",
        "EEIFIED_DIR = 'gs://{}/eeified_{}/'.format(BUCKET,MODEL_NAME)\n",
        "# change to your specific project\n",
        "PROJECT = 'ee-sandbox'\n",
        "\n",
        "# # You need to set the project before using the model prepare command.\n",
        "!earthengine set_project {PROJECT}\n",
        "!earthengine --no-use_cloud_api model prepare --source_dir {TF_DIR} --dest_dir {EEIFIED_DIR} --input {input_dict} --output {output_dict}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoT9jzn01V90"
      },
      "source": [
        "### Deploy to AI platform"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lppCYhh7SHC"
      },
      "source": [
        "import time\n",
        "\n",
        "MODEL_NAME = 'lc8_qa_model'\n",
        "VERSION_NAME = 'v' + str(int(time.time()))\n",
        "print('Creating version: ' + VERSION_NAME)\n",
        "\n",
        "!gcloud ai-platform versions create {VERSION_NAME} \\\n",
        "  --project {PROJECT} \\\n",
        "  --model {MODEL_NAME} \\\n",
        "  --origin {EEIFIED_DIR}/ \\\n",
        "  --runtime-version=1.14 \\\n",
        "  --framework \"TENSORFLOW\" \\\n",
        "  --python-version=3.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2t9PjbK3LBlS"
      },
      "source": [
        "!gcloud ai-platform versions describe $VERSION_NAME \\\n",
        "  --model $MODEL_NAME --project {PROJECT}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-CFdTLL1ZbP"
      },
      "source": [
        "## Running Model on EE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W09hfWWu8e93"
      },
      "source": [
        "# Use Landsat 8 SR data.\n",
        "# Get image based on location and time range\n",
        "geom = ee.Geometry.Point([-122.085,37.421]) # Google Campus\n",
        "image = ee.Image(l8sr.filterBounds(geom)\n",
        "  .filterDate(\"2019-06-01\",\"2020-01-01\")\n",
        "  .first()).select(BANDS)\n",
        "\n",
        "# Load the trained model and use it for prediction.\n",
        "model = ee.Model.fromAiPlatformPredictor(**{\n",
        "    'projectName': PROJECT,\n",
        "    'modelName': MODEL_NAME,\n",
        "    'version': VERSION_NAME,\n",
        "    'inputTileSize': [144,144],\n",
        "    'inputOverlapSize': [8,8],\n",
        "    'inputShapes': ee.Dictionary({\"array\":[6]}),\n",
        "    'proj': ee.Projection('EPSG:4326').atScale(30),\n",
        "    'fixInputProj': True,\n",
        "    'outputBands': {'qa': {\n",
        "        'type': ee.PixelType.float(),\n",
        "        'dimensions': 1\n",
        "      }\n",
        "    }\n",
        "});\n",
        "\n",
        "# run the predictions\n",
        "predictions = model.predictImage(image.toFloat().toArray())\n",
        "\n",
        "# find highest probability class\n",
        "predClasses = predictions \\\n",
        "  .arrayArgmax() \\\n",
        "  .arrayFlatten([['qa']]);\n",
        "\n",
        "# flatten probability array to image with bands  \n",
        "predProbs = predictions \\\n",
        "  .arrayFlatten([['cloud','shadow','snow','water','land','nodata']]).toFloat()\n",
        "\n",
        "# mask out the clear class\n",
        "predClasses = predClasses.updateMask(predClasses.neq(4));\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Safmk1shAAWq"
      },
      "source": [
        "# Use folium to visualize the input imagery and the predictions.\n",
        "# add Landsat 8 iamge to map\n",
        "mapid = image.getMapId({'bands': ['B7', 'B5', 'B3'], 'min': 0.05, 'max': 0.55, 'gamma': 1.5})\n",
        "map = folium.Map(location=[37.421,-122.085], zoom_start=12)\n",
        "folium.TileLayer(\n",
        "    tiles=mapid['tile_fetcher'].url_format,\n",
        "    attr='Google Earth Engine',\n",
        "    overlay=True,\n",
        "    name='Landsat Image',\n",
        "  ).add_to(map)\n",
        "\n",
        "# add predicted classes to map\n",
        "mapid = predClasses.getMapId({'min':0,'max':5,'palette':'white,gray,cyan,blue,green,black','opacity':0.7})\n",
        "folium.TileLayer(\n",
        "    tiles=mapid['tile_fetcher'].url_format,\n",
        "    attr='Google Earth Engine',\n",
        "    overlay=True,\n",
        "    name='QA Classes',\n",
        "  ).add_to(map)\n",
        "\n",
        "# add class probabilities to map\n",
        "mapid = predProbs.getMapId({'min':0,'max':1,'bands':\"cloud,shadow,water\",'opacity':0.5})\n",
        "folium.TileLayer(\n",
        "    tiles=mapid['tile_fetcher'].url_format,\n",
        "    attr='Google Earth Engine',\n",
        "    overlay=True,\n",
        "    name='Class Probabilities',\n",
        "  ).add_to(map)\n",
        "\n",
        "\n",
        "map.add_child(folium.LayerControl())\n",
        "map"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orqkrPMtBehk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}